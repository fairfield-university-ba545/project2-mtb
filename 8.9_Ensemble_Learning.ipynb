{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we combine our top 3 models employing Ensemble Learning, namely:\n",
    "* Logistic Regression\n",
    "* Random Forest Classifier\n",
    "* XGBoost Classifer\n",
    "\n",
    "_Note:_ pipeline 2 dataset used with overall best results for average weighted f1-score and average auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import pandas as pd\n",
    "df_pipeline2 = pd.read_csv(\"pipeline_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features and Target variables\n",
    "X = df_pipeline2.iloc[:, :-1] # Features is all columns in the dataframe except the last column\n",
    "Y = df_pipeline2.iloc[:, -1] # Target is the last column in the dataframe: 'Revenue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "All the weighted f1 results:\n",
      "--------------------------------------------------------------------------\n",
      "[0.95414527 0.92910042 0.93849473 0.94086302 0.89193247 0.86825904\n",
      " 0.86778652 0.85381105 0.87176977 0.85355366] \n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "Average of all the weighted f1 results:\n",
      "--------------------------------------------------------------------------\n",
      "0.8969715957082037 \n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "All the AUC scores:\n",
      "--------------------------------------------------------------------------\n",
      "[0.98717385 0.97446085 0.98449731 0.97407905 0.91533097 0.87751228\n",
      " 0.88680978 0.87788358 0.8809565  0.86928115] \n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "Average of all the AUC scores:\n",
      "--------------------------------------------------------------------------\n",
      "0.922798531796707 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "import warnings\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=2019)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "\n",
    "# note pipeline 2 hyper-parameters for LogisticRegression\n",
    "model1 = LogisticRegression(C=4714.8663634573895, dual=False, fit_intercept=True, max_iter=40, multi_class='multinomial', penalty='l2', solver='sag')\n",
    "estimators.append(('logisticregression', model1))\n",
    "\n",
    "# note pipeline 2 hyper-parameters for Random Forest\n",
    "model2 = RandomForestClassifier(bootstrap= False, criterion='entropy', max_depth=20, max_features='auto', min_samples_leaf=1, min_samples_split=2, n_estimators=210, random_state=2019)\n",
    "estimators.append(('randomforestclassifier', model2))\n",
    "\n",
    "# note pipeline 2 hyper-parameters for XGBoost\n",
    "model3 = xgb.XGBClassifier(colsample_bytree=1.0, gamma=6, max_depth=4, min_child_weight=1, subsample=1.0)\n",
    "estimators.append(('xgboostclassifier', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "#ensemble = VotingClassifier(estimators)\n",
    "\n",
    "# to calculate the roc_auc as a scoring metric we need to set voting equal to soft\n",
    "# source: https://www.oreilly.com/library/view/machine-learning-for/9781783980284/47c32d8b-7b01-4696-8043-3f8472e3a447.xhtml\n",
    "# In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. \n",
    "# In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels.\n",
    "\n",
    "# In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. \n",
    "# The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n",
    "\n",
    "# source: https://stackoverflow.com/questions/51465682/roc-auc-in-votingclassifier-randomforestclassifier-in-scikit-learn-sklearn\n",
    "ensemble = VotingClassifier(estimators,voting='soft')\n",
    "\n",
    "result_f1_weighted = model_selection.cross_val_score(ensemble, X, Y, cv=kfold,  scoring='f1_weighted')\n",
    "result_auc_weighted = model_selection.cross_val_score(ensemble, X, Y, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('All the weighted f1 results:')\n",
    "print('--------------------------------------------------------------------------')\n",
    "print(result_f1_weighted, '\\n')\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Average of all the weighted f1 results:')\n",
    "print('--------------------------------------------------------------------------')\n",
    "print(result_f1_weighted.mean(), '\\n')\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('All the AUC scores:')\n",
    "print('--------------------------------------------------------------------------')\n",
    "print(result_auc_weighted, '\\n')\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Average of all the AUC scores:')\n",
    "print('--------------------------------------------------------------------------')\n",
    "print(result_auc_weighted.mean(), '\\n')\n",
    "\n",
    "# reset the warnings for future code\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Take-away: </b>\n",
    "<br> The effect of the Voting Ensemble in this case did <b> NOT </b> result in a major improvement.\n",
    "* The average weighted f1 score increased from our best model Random Forest with 0.8914 to 0.8970\n",
    "* The average AUC score stayed approximately the same ~0.923"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
